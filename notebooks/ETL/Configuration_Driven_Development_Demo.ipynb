{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea295b6-23d7-416f-99f6-0c6a0991b424",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    BooleanType,\n",
    "    TimestampType,\n",
    "    DoubleType,\n",
    ")\n",
    "\n",
    "# Function to read YAML configuration from DBFS\n",
    "def read_config_from_dbfs(dbfs_path):\n",
    "    with open(dbfs_path, 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "# Path to your YAML configuration in DBFS\n",
    "config_path = '<Location of config file>'\n",
    "configs = read_config_from_dbfs(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2426eae-a1f7-4c4f-84f1-547fa46353ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to parse the schema from YAML configuration\n",
    "def parse_schema(schema_config):\n",
    "    type_mapping = {\n",
    "        \"IntegerType\": IntegerType(),\n",
    "        \"StringType\": StringType(),\n",
    "        \"DoubleType\": DoubleType(),\n",
    "        \"TimestampType\": TimestampType()\n",
    "    }\n",
    "    fields = [StructField(field['name'], type_mapping[field['type']], True) for field in schema_config]\n",
    "    return StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bb954a9-5118-4431-91c4-0aa0a5402b93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract client1 configuration and schema\n",
    "client1_config = configs['client1']\n",
    "schema = parse_schema(client1_config['schema'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4971553b-73a7-4ecd-8417-6f3389107839",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to apply transformations\n",
    "def transform_data(df, transformations):\n",
    "    for transformation in transformations:\n",
    "        if transformation['type'] == 'filter':\n",
    "            df = df.filter(transformation['expression'])\n",
    "        elif transformation['type'] == 'withColumn':\n",
    "            df = df.withColumn(transformation['column'], expr(transformation['expression']))\n",
    "        # Extend with more transformation types as needed\n",
    "    return df\n",
    "\n",
    "# Function to process each client's data\n",
    "def process_client_data(client_id, client_config):\n",
    "    df = spark.read.format(client_config['format']).schema(schema).options(**client1_config['options']).load(client_config['path'])\n",
    "    display(df)\n",
    "    transformed_df = transform_data(df, client_config['transformations'])\n",
    "    transformed_df.write.mode('overwrite').format(client_config['output_format']).save(client_config['output_path'])\n",
    "    print(f\"Processed data for client {client_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46627cce-4062-4b55-a41a-16301db079c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "process_client_data(\"client1\", client1_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ded3888-dd30-4213-a676-667f69e9c9c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/mnt/data/out/output/client_1_data*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9afa1e37-3c37-4093-b96e-d6dec5a24495",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Configuration_Driven_Development_Demo",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
